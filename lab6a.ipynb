{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6a: Decision Tree Classifiers\n",
    "\n",
    "In this lab, you will be analyzing the implementation of a simple decision tree classifier. The goal of Lab 6a is to help you understand  decision tree design.  This understanding will help you effectively optimize decision tree hyperparameters when modeling real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Review\n",
    "### What is a Decision Tree?\n",
    "A *Decision Tree* is  tree-shaped graph or model of decisions used to determine a course of action or show a statistical probability.  \n",
    "\n",
    "In context of supervised machine learning two types of decision tree are commonly employed:\n",
    "\n",
    "- **Classification Tree:** A decision tree that performs classification (predicts a categorical response).\n",
    "- **Regression Tree:** A decision tree that performs regression (predicts a numeric response).\n",
    "\n",
    "Though it is possible to have have hybrid decision trees that provide both types of responses.\n",
    "\n",
    "Below is a classification tree for classifying patients as either \"Low risk\" or \"High risk\" for a heart attack based on their medical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://onlinecourses.science.psu.edu/stat857/sites/onlinecourses.science.psu.edu.stat857/files/lesson13/image_01.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification works as follows, starting at the top of diagram (i.e., the root of the tree), and check whether or not the medical information present satisfies the _rule_ associated with each circle (i.e., _internal node_ or simply _node_) until we reach a square (i.e., _terminal node_, _leaf node_, or simply _leaf_). The patients class (\"Low risk\", or \"High risk\") is then provided by the _label_ associated with that square.\n",
    "\n",
    "When considered in the context of a set of data (e.g., training data), each _internal node_'s decision rule splits the set of data into two subsets, therefore _internal node_'s are also sometimes referred to as _split points_.\n",
    "\n",
    "An beautiful presentation on how ML Decision Trees work, can be found here [A Visual Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "For the classification tree below, answer the following questions:\n",
    "1. How many _internal nodes_ and _leaf nodes_ are need to represent the tree below.\n",
    "2. What decision _rules_ are present?\n",
    "3. Is all the feature data ordinal or is some it categorical?\n",
    "4. What are the possible _labels_ the tree can produce?\n",
    "<img src=\"images/credit_tree.png\" width=500>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pythonic tree class is implemented below. Skim the code below to get a general understanding of how you can code a decision tree in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data1030Tree:\n",
    "    \"\"\"An empty base class. Used to implement attributes shared by\n",
    "    Data1030Node and Data1030Leaf.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class Data1030Leaf(Data1030Tree):\n",
    "    \"\"\"A leaf in a decision tree\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    label: int\n",
    "       The label of the leaf.\n",
    "    \"\"\"\n",
    "    def __init__(self, label: int):\n",
    "        self.label = label\n",
    "\n",
    "class Data1030Node(Data1030Tree):\n",
    "    \"\"\"A node in a decision tree\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature: int\n",
    "        The integer index of the feature used to split the node.\n",
    "\n",
    "    threshold: float\n",
    "        The value to split the feature by. If the threshold is 2.5,\n",
    "        then all samples with values < 2.5 will be in the lower split\n",
    "        while all samples with values <= 2.5 will be in the upper split.\n",
    "\n",
    "    left_child: Data1030Tree\n",
    "        The subtree formed by samples below the threshold.\n",
    "\n",
    "    right_child: Data1030Tree\n",
    "        The subtree formed by samples above the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 feature: int,\n",
    "                 threshold: float,\n",
    "                 left_child: Data1030Tree,\n",
    "                 right_child: Data1030Tree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_string(dtree: Data1030Tree, depth=0):\n",
    "    \"\"\"Prints the tree in a human readable form.\n",
    "    \"\"\"\n",
    "    if isinstance(dtree, Data1030Leaf):\n",
    "        tree_str = '\\t' * depth\n",
    "        tree_str += 'Leaf: [Label] = {}\\n'.format(dtree.label)\n",
    "        return tree_str\n",
    "    \n",
    "    if isinstance(dtree, Data1030Node):\n",
    "        tree_str = ''\n",
    "\n",
    "        tree_str += '\\t' * depth\n",
    "        tree_str += 'Node: [Feature {}] < {}\\n'.format(dtree.feature, dtree.threshold)\n",
    "        tree_str += tree_to_string(dtree.left_child, depth=depth + 1)\n",
    "\n",
    "        tree_str += '\\t' * depth\n",
    "        tree_str += 'Node: [Feature {}] >= {}\\n'.format(dtree.feature, dtree.threshold)\n",
    "        tree_str += tree_to_string(dtree.right_child, depth=depth + 1)\n",
    "        return tree_str\n",
    "        \n",
    "# Sample points x, with x[3] <= 10 are categorized to label 2 by example_tree\n",
    "# Sample points x, with x[3] > 10 are categorized to label 1 by example_tree\n",
    "example_tree = Data1030Node(1,-3, Data1030Node(3, 10, Data1030Leaf(2), Data1030Leaf(1)), Data1030Leaf(3))\n",
    "print(tree_to_string(example_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to completely understand the details of the above implementation but do note\n",
    "that leafs and nodes are distinguished. As we traverse a decision tree, we are either\n",
    "\"splitting\" based on a rule (like Age < 62.5) or giving the data a label (like High Risk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## How do we construct the \"right\" tree?\n",
    "\n",
    "Given a tree, we know how to classify an observation. So how do we create the right tree?\n",
    "\n",
    "Creating a decision tree requires selecting input variables and split points on those variables until a suitable tree is constructed.\n",
    "\n",
    "There are many algorithms for doing this, each with tradeoffs when it comes to memory, generalizability, and performance.\n",
    "\n",
    "Breiman's original Classification and Regression Trees (CART) algorithm provides a good starting point. CART is a \"greedy algorithm\" that recursively splits the feature space rectilinearly until most of the data in a given hyperrectangle (box) has a certain label.  \n",
    "\n",
    "The algorithm is greedy in that the decision rule (feature and cut-off criteria) selected at each split point is the one that best minimizes a cost function applied to the current data. There is no back-tracking or look-ahead.\n",
    "\n",
    "If you run CART on a 2-D classification problem (i.e. one with two continuous features), and plot the decision boundaries for the resulting classification tree, you will get something like the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-choice/image5.png\"\n",
    "     width=300>\n",
    "     \n",
    "In the classification tree above, note how each _leaf_ in the decision tree below corresponds to a different _box_ in the decision boundary plot above it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Suppose that we swapped the positions of the node __Income < 61k__ and the node __Age < 27__ in the tree above. \n",
    "\n",
    "1. Would any data points in the above graph be classified differently? 2. Describe what would change about green lines in the coordinate graph above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breiman's CART Algorithm\n",
    "To _greedily_ create the tree, CART does the following:\n",
    "\n",
    "1. (Stopping Condition) Check if the input data, X, are mostly of a certain label, if so create a leaf with that label.\n",
    "2. (Split-node Selection) If not, split the data based on the rule (ex. Age < 27) that best minimizes splitting costs. \n",
    "3. Run steps 1 and 2 recursively on each split.\n",
    "\n",
    "_Gini impurity_, $G(y)$, is defined as as follows:\n",
    "\n",
    "Given a labeled data set (X,y) with $k$ possible class labels. Let $p_i$ be the proportion of observations (y) with class label $i$.\n",
    "\n",
    "$$G(y) = \\sum_{i = 1}^{k}p_i (1 - p_i) = 1 - \\sum_{i = 1}^{k}p_i^2$$\n",
    "\n",
    "A _weighted sum_ of Gina impurity measures is used to evaluate the cost of each possible split.  Given a split of (X,y) into $(X^1,y^1)$ and $(X^2,y^2)$, the $cost$ of this split is \n",
    "$$cost(y^1, y^2) = \\frac{|y^1|}{|y|}g(y^1)| + \\frac{|y^2|}{|y|}g(y^2))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Implementation\n",
    "The Data1030TreeBuilder code below contains a `.build` method \n",
    "that implements CART that allows the user to select \n",
    "either 'gini' or 'entropy' for decision criteria evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Data1030TreeBuilder:\n",
    "    \"\"\"Constructs a decision tree (a collection of connected\n",
    "    Data1030Node and Data1030Leaf objects)\n",
    "\n",
    "    To construct a decision tree, instantiate a\n",
    "    Data1030TreeBuilder object with no parameters\n",
    "    and then call the .build method with the training\n",
    "    data and training labels as arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self,\n",
    "              X: np.array,\n",
    "              y: np.array,\n",
    "              criterion: str) -> Data1030Tree:\n",
    "        \"\"\"Returns a Data1030 tree from the input data.\"\"\"\n",
    "        # Checks whether or not all of the sample labels are the same\n",
    "        if np.all(y == y[0]):\n",
    "            return Data1030Leaf(y[0])\n",
    "\n",
    "        # Finds the pair (attribute, split_value) with the highest\n",
    "        # information gain\n",
    "        attribute, threshold = self.best_split(X, y, criterion)\n",
    "\n",
    "        # Calls self.build on row pairs in X, y which are\n",
    "        # below the threshold\n",
    "        left_child = self.build(X[X[:, attribute] < threshold],\n",
    "                                y[X[:, attribute] < threshold],\n",
    "                                criterion)\n",
    "\n",
    "        # Calls self.build on row pairs in X, y which are\n",
    "        # above or equal to the threshold\n",
    "        right_child = self.build(X[X[:, attribute] >= threshold],\n",
    "                                 y[X[:, attribute] >= threshold],\n",
    "                                 criterion)\n",
    "\n",
    "        return Data1030Node(attribute, threshold, left_child, right_child)\n",
    "\n",
    "    def best_split(self,\n",
    "                   X: np.array,\n",
    "                   y: np.array,\n",
    "                   criterion='gini') -> (int, float):\n",
    "        \"\"\"Returns the attribute and threshold that result in the\n",
    "        optimal split. This occurs when the gini impurity or\n",
    "        entropy of the resulting subsets is minimized.\n",
    "        \"\"\"\n",
    "        best_attribute = -1\n",
    "        best_threshold = - 1\n",
    "        best_impurity_loss = 1\n",
    "        for attribute in range(X.shape[1]):\n",
    "            # The lowest unique element cannot be a threshold value\n",
    "            # as the split must split X into 2 non-empty collections\n",
    "            possible_thresholds = np.unique(X[:, attribute])[1:]\n",
    "            for threshold in possible_thresholds:\n",
    "                # Chooses the criterion to use\n",
    "                if criterion is 'gini':\n",
    "                    current_impurity_loss = self.gini(X,\n",
    "                                                      y,\n",
    "                                                      attribute,\n",
    "                                                      threshold)\n",
    "                elif criterion is 'entropy':\n",
    "                    current_impurity_loss = self.entropy(X,\n",
    "                                                         y,\n",
    "                                                         attribute,\n",
    "                                                         threshold)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        '{} is not a valid criterion'.format(criterion))\n",
    "                # Checks to see whether the current split results in lower loss\n",
    "                if current_impurity_loss < best_impurity_loss:\n",
    "                    best_attribute = attribute\n",
    "                    best_threshold = threshold\n",
    "                    best_impurity_loss = current_impurity_loss\n",
    "\n",
    "        return best_attribute, best_threshold\n",
    "\n",
    "    def gini(self,\n",
    "             X: np.array,\n",
    "             y: np.array,\n",
    "             attribute: int,\n",
    "             threshold: float) -> float:\n",
    "        \"\"\"Calculates the weighted sum of the gini impurities of the candidate\n",
    "        left and right children.\n",
    "        \"\"\"\n",
    "        # Let arr be an Numpy array of integers\n",
    "        # Calling arr[arr < 5] returns all elements in less than 5\n",
    "        # In the above scenario, arr < 5 is a \"mask\"\n",
    "        masks = (X[:, attribute] < threshold, X[:, attribute] >= threshold)\n",
    "        gini_sum = 0\n",
    "        for mask in masks:\n",
    "            # The line below counts the number of elements in each\n",
    "            # output class/label\n",
    "            counts = np.unique(y[mask], return_counts=True)[1]\n",
    "            proportions = counts / len(y[mask])\n",
    "            gini_impurity = 1 - np.dot(proportions, proportions)\n",
    "            # The weight is the proportion of elements in the subset, y[mask],\n",
    "            gini_weight = len(y[mask]) / len(y)\n",
    "            gini_sum += gini_weight * gini_impurity\n",
    "        return gini_sum\n",
    "\n",
    "    def entropy(self,\n",
    "                X: np.array,\n",
    "                y: np.array,\n",
    "                attribute: int,\n",
    "                threshold: float) -> float:\n",
    "        \"\"\"Calculates the weighted sum of the entropies of the candidate\n",
    "        left and right children.\n",
    "        \n",
    "        If you wish to consult information theory resources,\n",
    "        this is the conditional entropy:\n",
    "        \n",
    "            H(X|RULE) = p(RULE=True) * H(X|Rule=True) + p(RULE=False) * H(X|Rule=False)\n",
    "        \"\"\"\n",
    "        # TASK 3 HERE\n",
    "        # BEGIN SOLUTION\n",
    "        masks = (X[:, attribute] < threshold, X[:, attribute] >= threshold)\n",
    "        entropy_sum = 0\n",
    "        for mask in masks:\n",
    "            counts = np.unique(y[mask], return_counts=True)[1]\n",
    "            proportions = counts / len(y[mask])\n",
    "            conditional_entropy = -np.dot(proportions, np.log2(proportions))\n",
    "            entropy_weight = len(y[mask]) / len(y)\n",
    "            entropy_sum += entropy_weight * conditional_entropy\n",
    "        return entropy_sum\n",
    "        # END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Explain how best_split operates.  How is the returned \n",
    "(attribute index, threshold) found? Include a description of the number of threshold values that are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Entropy Measure\n",
    "_Entropy_, $E(y)$ is another common measure used for split-point selection.  It is defined as follows:\n",
    "\n",
    "$$E(y) = \\sum_{i = 0}^{k} p_i \\log \\frac{1}{p_i}$$\n",
    "\n",
    "where, as before, $p_i$ is the proportion of observations (y) with class label $i$.\n",
    "\n",
    "The more \"variable\" a random variable is, the higher its value of _gini impurity_ and _entropy_ will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Complete the `.entropy()` method in the `Data1030TreeBuilder` code above so that returns the cost of a split using Entropy. Use the implementation of Gini impurity to help you. Below is some test code to see if you have it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_entropy():\n",
    "    builder = Data1030TreeBuilder()\n",
    "    y = np.random.randint(2, size=10)*10\n",
    "    print('y = ', y)\n",
    "    X = np.column_stack((np.random.randint(2, size=10)*10, y)) # Second column of X contains y\n",
    "    print('X = ', X)\n",
    "    cost_split_1 = builder.entropy(X, y, 0, 5) # Try splitting using X[:,0] < 5\n",
    "    cost_split_2 = builder.entropy(X, y, 1, 5) # Try splitting using X[:,1] < 5\n",
    "    print('cost_split_1 =', cost_split_1)\n",
    "    print('cost_split_2 =', cost_split_2)\n",
    "    assert cost_split_1 >= 0 # cost of random split  \n",
    "    assert cost_split_2 == 0 # cost of a perfect split is zero\n",
    "\n",
    "    \n",
    "test_entropy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a fully functional TreeClassfier, with .fit() and .predict() methods.  Notice how clean the design is.  It may seem surprising how powerful they are, given how straightforward they are to work with.  Soon, you will learn about Deep Learning models that have even simpler implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data1030TreeClassifier(Data1030TreeBuilder):\n",
    "    \"\"\"A simple decision tree classifier based on sklearn's design\n",
    "\n",
    "    To use the class, instantiate a Data1030TreeClassifier object\n",
    "    and then call the .fit method to train the model. \n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    tree_:\n",
    "        The underlying DATA1030Tree used to get predicted labels. Set to \n",
    "        none if the tree is not trained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tree_ = None\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array, criterion='gini') -> None:\n",
    "        \"\"\"Trains the model on the training data X and\n",
    "        training labels y. \n",
    "\n",
    "        Categorical features should be encoded as integers.\n",
    "        The output labels should be encoded as integers.\n",
    "        \"\"\"\n",
    "        self.tree_ = self.build(X, y, criterion)\n",
    "\n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        \"\"\"Predicts the labels y-hat of of the given inputs X.\n",
    "        \n",
    "        The input features X should be encoded in the same\n",
    "        format as the training features.\n",
    "        \"\"\"\n",
    "        if self.tree_ is None:\n",
    "            msg = ('This decision tree has not been fitted. Call \"fit\" with '\n",
    "                   'appropriate arguments before using this method.')\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        predictions = np.zeros(shape=len(y_train), dtype=np.int)\n",
    "        for i in range(len(X)):\n",
    "            cursor = self.tree_\n",
    "            # Traverses the tree according to the rule in the node\n",
    "            # until cursor reaches a leaf\n",
    "            while isinstance(cursor, Data1030Node):\n",
    "                if X[i, cursor.feature] < cursor.threshold:\n",
    "                    cursor = cursor.left_child\n",
    "                else:\n",
    "                    cursor = cursor.right_child\n",
    "            predictions[i] = cursor.label\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART can also be used to do regression. [This guide](https://sadanand-singh.github.io/posts/treebasedmodels/) is a good introduction to CART for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Applying _sklearn_ decision trees\n",
    "\n",
    "In practice, you will spend a lot more time prototyping ML models with _sklearn_ than coding models from scratch. However _sklearn's_ implementation of CART is a bit hard to handle. For example, `sklearn.tree.DecisionTreeClassifier` has 13 parameters!\n",
    "\n",
    "If you tune those parameters improperly, you likely will end up with an _overfitted_ model like the one graphed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://stephanie-w.github.io/brainscribble/figure/classification-algorithms-on-iris-dataset_48_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Before we dive into the parameters, install the `python-graphviz` conda package by typing the following in your terminal:\n",
    "\n",
    "`conda install python-graphviz`\n",
    "\n",
    "Once you have installed `python-graphviz`, _restart your kernel_ and run the code block to create a diagram of an iris classification tree.\n",
    "\n",
    "**Take some time to understand what each row in each node represents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# Loads your feature data to X and label data to y\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Fits your decision tree to the iris data\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Graphs your tree\n",
    "dot_data = export_graphviz(clf, \n",
    "                           feature_names = iris.feature_names, # optional \n",
    "                           class_names  = iris.target_names, # optional\n",
    "                           out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of important `DecisionTreeClassifier` parameters from [dataaspirant.com](dataaspirant.com). It is a good idea to be familiar with many of these.\n",
    "\n",
    "* __criterion:__ It defines the function to measure the quality of a split. Sklearn supports “gini” criteria for Gini Index & “entropy” for Information Gain. By default, it takes “gini” value.\n",
    "* __splitter:__ It defines the strategy to choose the split at each node. Supports “best” value to choose the best split & “random” to choose the best random split. By default, it takes “best” value.\n",
    "* __max_features:__ It defines the no. of features to consider when looking for the best split. We can input integer, float, string & None value.\n",
    "    If an integer is inputted then it considers that value as max features at each split.\n",
    "    If float value is taken then it shows the percentage of features at each split.\n",
    "    If “auto” or “sqrt” is taken then max_features=sqrt(n_features).\n",
    "    If “log2” is taken then max_features= log2(n_features).\n",
    "    If None, then max_features=n_features. By default, it takes “None” value.\n",
    "* __max_depth:__ The max_depth parameter denotes maximum depth of the tree. It can take any integer value or None. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. By default, it takes “None” value.\n",
    "* __min_samples_split:__ This tells above the minimum no. of samples reqd. to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no. If float, then it shows percentage. By default, it takes “2” value.\n",
    "* __min_samples_leaf:__ The minimum number of samples required to be at a leaf node. If an integer value is taken then consider min_samples_leaf as the minimum no. If float, then it shows percentage. By default, it takes “1” value.\n",
    "* __max_leaf_nodes:__ It defines the maximum number of possible leaf nodes. If None then it takes an unlimited number of leaf nodes. By default, it takes “None” value.\n",
    "* __min_impurity_decrease:__ It defines the threshold for early stopping tree growth. A node will split if its impurity is above the threshold otherwise it is a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Categorize the above parameters into (1) those that directly affect _when_ the tree stops splitting (2) those that directly affect _how_ the tree splits."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"When\" parameters:\n",
    "# YOUR ANSWER HERE\n",
    "\n",
    "\"How\" parameters:\n",
    "# YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "If you want to reduce the overfitting of your model would you increase or decrease the following parameters?\n",
    "\n",
    "* max_depth\n",
    "* min_samples_split\n",
    "* min_samples_leaf\n",
    "* max_leaf_nodes\n",
    "* min_impurity_decrease"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Run the code block below and compare the classification tree here with the one above.  \n",
    "\n",
    "Answer the following:\n",
    "1. Is the training accuracy here below _better_ or _worse_ than the one above?  \n",
    "2. What is the training accuracy for the try below?  What is it for the tree above?\n",
    "\n",
    "Next, try adjusting the parameters of DecisionTreeClassifier instance assigned to `clf2` to see how your tree responds to changes to each parameter.  The parameters are pretty self-explanatory, but feel free to check the sklearn [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) documentation as well.\n",
    "\n",
    "Note: if you get an error about `min_impurity_decrease` upgrade your sklearn-learn via\n",
    "```\n",
    "conda install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth=2,\n",
    "                              min_samples_split=2,\n",
    "                              max_leaf_nodes=10,\n",
    "                              min_impurity_decrease=0)\n",
    "clf2.fit(X, y)\n",
    "\n",
    "# Graphs your tree\n",
    "dot_data2 = export_graphviz(clf2, \n",
    "                            feature_names = iris.feature_names, # optional \n",
    "                            class_names  = iris.target_names, # optional\n",
    "                            out_file=None) \n",
    "graph2 = graphviz.Source(dot_data2) \n",
    "graph2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "\n",
    "True or False:\n",
    "\n",
    "1. Overfitting isn't a problem with decision trees.\n",
    "2. It is always possible to perfectly overfit the data using Decision trees.\n",
    "3. Do you need to normalize numerical features before calling DecisionTreeClassifier.fit?\n",
    "4. Because CART greedily chooses the best split, will a decision tree built by CART always have a minimum total cost? (i.e. will they be optimal in that sense?)\n",
    "\n",
    "If you are stuck read the _sklearn_ [User Guide](http://scikit-learn.org/stable/modules/tree.html) on decision trees."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Take Home Messages\n",
    "* Decision trees are typically binary trees. Each node has a rule (like age < 10) and each leaf has a label (e.g. went to burning man = True).\n",
    "* _sklearn_ decision trees have many optional parameters. How one adjusts them can help deal with overfitting.\n",
    "* _sklearn_ decision trees do not require you to preprocess your data much and can be visualized with `graphviz`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
